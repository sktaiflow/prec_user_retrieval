{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233ae0d5-b631-4211-903b-bef05c3a3e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:15.478713Z",
     "iopub.status.busy": "2024-06-28T15:33:15.477778Z",
     "iopub.status.idle": "2024-06-28T15:33:15.604372Z",
     "shell.execute_reply": "2024-06-28T15:33:15.603044Z",
     "shell.execute_reply.started": "2024-06-28T15:33:15.478633Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skt.vault_utils import get_secrets\n",
    "proxies = get_secrets('proxies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f8eb57-d288-4990-841f-c5d103a78384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:15.607804Z",
     "iopub.status.busy": "2024-06-28T15:33:15.607138Z",
     "iopub.status.idle": "2024-06-28T15:33:16.905471Z",
     "shell.execute_reply": "2024-06-28T15:33:16.904115Z",
     "shell.execute_reply.started": "2024-06-28T15:33:15.607746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skt.gcp import (\n",
    "    PROJECT_ID,\n",
    "    bq_insert_overwrite,\n",
    "    bq_to_df,\n",
    "    bq_to_pandas,\n",
    "    df_to_bq_table,\n",
    "    get_bigquery_client,\n",
    "    bq_table_exists,\n",
    "    get_max_part,\n",
    "    load_query_result_to_table,\n",
    "    pandas_to_bq,\n",
    "    pandas_to_bq_table,\n",
    "    load_bigquery_ipython_magic,\n",
    "    get_bigquery_client,\n",
    "    _print_query_job_results,\n",
    "    load_query_result_to_partitions\n",
    "    \n",
    ")\n",
    "\n",
    "from skt.ye import (\n",
    "    get_hdfs_conn,\n",
    "    get_spark,\n",
    "    hive_execute,\n",
    "    hive_to_pandas,\n",
    "    pandas_to_parquet,\n",
    "    slack_send,\n",
    "    get_secrets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16806681-24d0-4c30-af2e-9d6e1f5e3339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:16.921452Z",
     "iopub.status.busy": "2024-06-28T15:33:16.920740Z",
     "iopub.status.idle": "2024-06-28T15:33:16.926447Z",
     "shell.execute_reply": "2024-06-28T15:33:16.925402Z",
     "shell.execute_reply.started": "2024-06-28T15:33:16.921394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a58195-2e91-4a9c-abc1-b912551cda55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:16.928490Z",
     "iopub.status.busy": "2024-06-28T15:33:16.927930Z",
     "iopub.status.idle": "2024-06-28T15:33:16.937868Z",
     "shell.execute_reply": "2024-06-28T15:33:16.937021Z",
     "shell.execute_reply.started": "2024-06-28T15:33:16.928460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = proxies['http']\n",
    "os.environ['https_proxy'] = proxies['https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a757ba5-f5b1-432d-b2b8-63d4b955edd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:16.939211Z",
     "iopub.status.busy": "2024-06-28T15:33:16.938846Z",
     "iopub.status.idle": "2024-06-28T15:33:16.948596Z",
     "shell.execute_reply": "2024-06-28T15:33:16.947727Z",
     "shell.execute_reply.started": "2024-06-28T15:33:16.939181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f25b522-4abc-4c8f-9bec-fd22047fb448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:16.950238Z",
     "iopub.status.busy": "2024-06-28T15:33:16.949790Z",
     "iopub.status.idle": "2024-06-28T15:33:18.489388Z",
     "shell.execute_reply": "2024-06-28T15:33:18.487918Z",
     "shell.execute_reply.started": "2024-06-28T15:33:16.950198Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig\n",
    ")\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538419b-674d-46ff-855d-7e2bafdef678",
   "metadata": {},
   "source": [
    "# EVENT EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec09a01-bd22-4ea0-9463-5e6b356ecffc",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73e24cb5-0897-47c2-a835-d091adf2e132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T06:53:53.514250Z",
     "iopub.status.busy": "2024-06-28T06:53:53.513491Z",
     "iopub.status.idle": "2024-06-28T06:53:53.520739Z",
     "shell.execute_reply": "2024-06-28T06:53:53.519464Z",
     "shell.execute_reply.started": "2024-06-28T06:53:53.514191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tables(table_nm:str):\n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM adot_reco_dev.{table_nm}\n",
    "    \"\"\"\n",
    "    return bq_to_pandas(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d45fd11-4eb9-47bc-a6c0-e847700f5a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T06:53:53.775337Z",
     "iopub.status.busy": "2024-06-28T06:53:53.774649Z",
     "iopub.status.idle": "2024-06-28T06:53:57.348379Z",
     "shell.execute_reply": "2024-06-28T06:53:57.347112Z",
     "shell.execute_reply.started": "2024-06-28T06:53:53.775260Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: exception on print statistics\n",
      "unsupported operand type(s) for /: 'NoneType' and 'int'\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "data_df = get_tables(table_nm=\"adotUser_tdeal_profile_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a2e7e54-f5d2-4512-97d5-7da337ad1a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T06:53:57.351130Z",
     "iopub.status.busy": "2024-06-28T06:53:57.350626Z",
     "iopub.status.idle": "2024-06-28T06:53:57.355350Z",
     "shell.execute_reply": "2024-06-28T06:53:57.354462Z",
     "shell.execute_reply.started": "2024-06-28T06:53:57.351091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a757a9de-f0fb-4622-b987-07921ccb3761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T06:53:57.357095Z",
     "iopub.status.busy": "2024-06-28T06:53:57.356649Z",
     "iopub.status.idle": "2024-06-28T06:53:57.382310Z",
     "shell.execute_reply": "2024-06-28T06:53:57.381533Z",
     "shell.execute_reply.started": "2024-06-28T06:53:57.357058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile_dict = data_df.set_index(\"luna_id\").to_dict()['tdeal_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e23f5b-29ba-4eb0-8ff7-756a084b4b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:40.920090Z",
     "iopub.status.busy": "2024-06-28T01:54:40.919790Z",
     "iopub.status.idle": "2024-06-28T01:54:40.929768Z",
     "shell.execute_reply": "2024-06-28T01:54:40.929066Z",
     "shell.execute_reply.started": "2024-06-28T01:54:40.920068Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = []\n",
    "luna_ids = []\n",
    "for luna_id, profile in profile_dict.items():\n",
    "    profiles.append(profile)\n",
    "    luna_ids.append(luna_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5f979-74ae-436a-b8a8-300afad57ee5",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d4495d-8277-4bb1-a1f8-56a214d439f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:18.770680Z",
     "iopub.status.busy": "2024-06-28T15:33:18.769987Z",
     "iopub.status.idle": "2024-06-28T15:33:20.349052Z",
     "shell.execute_reply": "2024-06-28T15:33:20.347546Z",
     "shell.execute_reply.started": "2024-06-28T15:33:18.770621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('BM-K/KoDiffCSE-RoBERTa')\n",
    "tokenizer = AutoTokenizer.from_pretrained('BM-K/KoDiffCSE-RoBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf013d95-0389-4bc9-a6f5-326a1ec5070b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:20.352570Z",
     "iopub.status.busy": "2024-06-28T15:33:20.351597Z",
     "iopub.status.idle": "2024-06-28T15:33:20.653157Z",
     "shell.execute_reply": "2024-06-28T15:33:20.652413Z",
     "shell.execute_reply.started": "2024-06-28T15:33:20.352508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b81386c-83e4-405b-b7d5-0e2f0d182421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:20.654494Z",
     "iopub.status.busy": "2024-06-28T15:33:20.654260Z",
     "iopub.status.idle": "2024-06-28T15:33:20.657939Z",
     "shell.execute_reply": "2024-06-28T15:33:20.657387Z",
     "shell.execute_reply.started": "2024-06-28T15:33:20.654472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba69d70-6266-4a3a-bfd1-ebbdb4874a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:33:20.659677Z",
     "iopub.status.busy": "2024-06-28T15:33:20.659389Z",
     "iopub.status.idle": "2024-06-28T15:33:20.672709Z",
     "shell.execute_reply": "2024-06-28T15:33:20.672177Z",
     "shell.execute_reply.started": "2024-06-28T15:33:20.659655Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_embedd(profiles:list, luna_ids:list, batch_size = 32, method=\"cls\"):\n",
    "    embedding_result = []\n",
    "    luna_id_result = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(profiles), batch_size):\n",
    "            batch = profiles[i:i+batch_size]\n",
    "            batch_luna_ids = luna_ids[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            #embeddings, _ = model(**inputs, return_dict=False)\n",
    "            if method =='cls':\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]  \n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "            elif method =='mean_pool':\n",
    "                embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings = embeddings.unsqueeze(1) # Shape: (batch_size, 768)\n",
    "            embedding_result.extend(embeddings.cpu().numpy())\n",
    "            luna_id_result.extend(batch_luna_ids)\n",
    "            \n",
    "    return embedding_result, luna_id_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba72170-a834-4cd0-8dc4-7bc2331a444e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:42.433958Z",
     "iopub.status.busy": "2024-06-28T01:54:42.433775Z",
     "iopub.status.idle": "2024-06-28T01:54:50.372839Z",
     "shell.execute_reply": "2024-06-28T01:54:50.371817Z",
     "shell.execute_reply.started": "2024-06-28T01:54:42.433940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_result, luna_id_result = batch_embedd(profiles, luna_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51815043-532a-4683-acea-9383bdf65480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:50.374739Z",
     "iopub.status.busy": "2024-06-28T01:54:50.374053Z",
     "iopub.status.idle": "2024-06-28T01:54:50.403550Z",
     "shell.execute_reply": "2024-06-28T01:54:50.402836Z",
     "shell.execute_reply.started": "2024-06-28T01:54:50.374710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(luna_id_result)):\n",
    "    result.append({\"profile_text\": profiles[i], \"profile_vector\": embedding_result[i].squeeze(0), \"index\":luna_id_result[i], 'source_domain':'tdeal', 'dt':'temp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df59295b-4470-40c4-a6ed-9561386a78fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:50.405402Z",
     "iopub.status.busy": "2024-06-28T01:54:50.405012Z",
     "iopub.status.idle": "2024-06-28T01:54:50.418114Z",
     "shell.execute_reply": "2024-06-28T01:54:50.417553Z",
     "shell.execute_reply.started": "2024-06-28T01:54:50.405376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][\"profile_vector\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e516d828-ddb2-4e9e-94cc-82e48d1d4817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:52.987893Z",
     "iopub.status.busy": "2024-06-28T01:54:52.987001Z",
     "iopub.status.idle": "2024-06-28T01:54:53.015144Z",
     "shell.execute_reply": "2024-06-28T01:54:53.014022Z",
     "shell.execute_reply.started": "2024-06-28T01:54:52.987829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57c736a5-a090-434b-ba7c-e479be80e22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:53.674827Z",
     "iopub.status.busy": "2024-06-28T01:54:53.674101Z",
     "iopub.status.idle": "2024-06-28T01:54:53.691604Z",
     "shell.execute_reply": "2024-06-28T01:54:53.690586Z",
     "shell.execute_reply.started": "2024-06-28T01:54:53.674766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_text</th>\n",
       "      <th>profile_vector</th>\n",
       "      <th>index</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>특징: 육아/아기,평소 헬스,테니스,건강식품,디지털/가전,스포츠/레저,화장품/미용 ...</td>\n",
       "      <td>[0.0702496, -0.02036032, -0.042267658, -0.0299...</td>\n",
       "      <td>APL00000DC538LWDCX6O</td>\n",
       "      <td>tdeal</td>\n",
       "      <td>temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>평소 기타스포츠용품,오토바이/스쿠터,디지털/가전,스포츠/레저,화장품/미용 관련 용품...</td>\n",
       "      <td>[0.041700225, -0.021257862, -0.024179967, -0.0...</td>\n",
       "      <td>APL00000D1JXIKIR3BI8</td>\n",
       "      <td>tdeal</td>\n",
       "      <td>temp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        profile_text  \\\n",
       "0  특징: 육아/아기,평소 헬스,테니스,건강식품,디지털/가전,스포츠/레저,화장품/미용 ...   \n",
       "1  평소 기타스포츠용품,오토바이/스쿠터,디지털/가전,스포츠/레저,화장품/미용 관련 용품...   \n",
       "\n",
       "                                      profile_vector                 index  \\\n",
       "0  [0.0702496, -0.02036032, -0.042267658, -0.0299...  APL00000DC538LWDCX6O   \n",
       "1  [0.041700225, -0.021257862, -0.024179967, -0.0...  APL00000D1JXIKIR3BI8   \n",
       "\n",
       "  source_domain    dt  \n",
       "0         tdeal  temp  \n",
       "1         tdeal  temp  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad998bf8-562d-4226-bc0c-d5bc12ee01e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:55.620632Z",
     "iopub.status.busy": "2024-06-28T01:54:55.619871Z",
     "iopub.status.idle": "2024-06-28T01:54:55.625703Z",
     "shell.execute_reply": "2024-06-28T01:54:55.624740Z",
     "shell.execute_reply.started": "2024-06-28T01:54:55.620573Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"skt-datahub\"\n",
    "db_name = \"adot_reco_dev\"\n",
    "table_name = \"tdeal_profile_vector_temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0357f911-764c-4e4b-a02b-9fd7ce1c4e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:54:57.311407Z",
     "iopub.status.busy": "2024-06-28T01:54:57.310619Z",
     "iopub.status.idle": "2024-06-28T01:55:07.723986Z",
     "shell.execute_reply": "2024-06-28T01:55:07.722489Z",
     "shell.execute_reply.started": "2024-06-28T01:54:57.311346Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas_to_bq(pd_df = df, destination=f\"{PROJECT_ID}.{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222c0ab-ba8d-4841-8345-b5a53b8739b1",
   "metadata": {},
   "source": [
    "# TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497d9ca5-be9c-4923-9a84-d5f9bc3fc872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:55:18.134171Z",
     "iopub.status.busy": "2024-06-28T01:55:18.133271Z",
     "iopub.status.idle": "2024-06-28T01:55:18.139996Z",
     "shell.execute_reply": "2024-06-28T01:55:18.138740Z",
     "shell.execute_reply.started": "2024-06-28T01:55:18.134107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile_vector_dict = dict()\n",
    "for res in result:\n",
    "    profile_vector_dict[res[\"profile_text\"]] = res[\"profile_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0ccad01c-6624-438c-b38f-b19b2a76afd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:42.952183Z",
     "iopub.status.busy": "2024-06-28T01:50:42.951757Z",
     "iopub.status.idle": "2024-06-28T01:50:42.963131Z",
     "shell.execute_reply": "2024-06-28T01:50:42.962014Z",
     "shell.execute_reply.started": "2024-06-28T01:50:42.952147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quality test\n",
    "query_set = [\"간단한 레슨과 특별한 경기로 골프를 경험해 보세요!\",\n",
    "             \"평소 건강체크에 관심이 많은 고객\",\n",
    "             \"깊은 수면 부족했던 어젯밤 오늘은 깊게 잠들 수 있게 요가로 릴랙스해 봐요\",\n",
    "             \"미워할 수 없는 동물들의 이야기를 함께 들여다볼까요?\",\n",
    "             \"골프존, 카카오골프 등 골프 관련 어플이나 웹페이지 경험이 있고, 골프존 마켓, 카카오프렌즈 골프, tdeal 등에서 골프 관련 제품 쇼핑 경험이 있으며, tmap에서 골프 관련 장소를 찾은 적이 있고, 골프 관련 통화내역이 존재하며, 에이닷 미디어에서 골프 관련 영상을 시청한 적이 있는 고객\"\n",
    "            ]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d334a9de-005a-4924-b1c4-10a187040fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:42.964958Z",
     "iopub.status.busy": "2024-06-28T01:50:42.964484Z",
     "iopub.status.idle": "2024-06-28T01:50:42.979195Z",
     "shell.execute_reply": "2024-06-28T01:50:42.978544Z",
     "shell.execute_reply.started": "2024-06-28T01:50:42.964916Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embedd(sentences:list, method='cls'):\n",
    "    result = dict()\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs  = model(**inputs, return_dict=True)\n",
    "    if method =='cls':\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  \n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        embeddings = embeddings.unsqueeze(1)\n",
    "    elif method =='mean_pool':\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        embeddings = embeddings.unsqueeze(1) # Shape: (batch_size, 768)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        result[sent] = embeddings[i][0].cpu()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ac07dc3b-86f2-44a4-94a4-78286e1482be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:42.980853Z",
     "iopub.status.busy": "2024-06-28T01:50:42.980505Z",
     "iopub.status.idle": "2024-06-28T01:50:43.008551Z",
     "shell.execute_reply": "2024-06-28T01:50:43.007797Z",
     "shell.execute_reply.started": "2024-06-28T01:50:42.980825Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_dict = embedd(query_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3e129873-82f9-47bc-b4ed-5b0c32b3b97a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:43.010683Z",
     "iopub.status.busy": "2024-06-28T01:50:43.010393Z",
     "iopub.status.idle": "2024-06-28T01:50:43.014539Z",
     "shell.execute_reply": "2024-06-28T01:50:43.013989Z",
     "shell.execute_reply.started": "2024-06-28T01:50:43.010664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_indices = random.sample(range(len(profiles)), 1000)\n",
    "random_profiles = [profiles[i] for i in random_indices] + [\"반려동물\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e8846eae-6af8-40fe-86a2-8d8d45739981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:43.015472Z",
     "iopub.status.busy": "2024-06-28T01:50:43.015279Z",
     "iopub.status.idle": "2024-06-28T01:50:43.025737Z",
     "shell.execute_reply": "2024-06-28T01:50:43.025204Z",
     "shell.execute_reply.started": "2024-06-28T01:50:43.015453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#profile_dict = embedd(random_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e3ea9120-de7e-4c8b-a1c4-44c3d2f116e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:43.026571Z",
     "iopub.status.busy": "2024-06-28T01:50:43.026393Z",
     "iopub.status.idle": "2024-06-28T01:50:43.038328Z",
     "shell.execute_reply": "2024-06-28T01:50:43.037772Z",
     "shell.execute_reply.started": "2024-06-28T01:50:43.026552Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k_profiles(query_dict, profile_dict, k=5):\n",
    "    # Convert dictionaries to tensors\n",
    "    query_strings = list(query_dict.keys())\n",
    "    profile_strings = list(profile_dict.keys())\n",
    "    \n",
    "    query_embeddings = torch.stack([torch.tensor(emb) for emb in query_dict.values()])\n",
    "    profile_embeddings = torch.stack([torch.tensor(emb) for emb in profile_dict.values()])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    #query_embeddings = query_embeddings / query_embeddings.norm(dim=1)[:, None]\n",
    "    #profile_embeddings = profile_embeddings / profile_embeddings.norm(dim=1)[:, None]\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = torch.mm(query_embeddings.squeeze(1), profile_embeddings.squeeze(1).t())\n",
    "\n",
    "    # Get top k similar profiles for each query\n",
    "    top_k_values, top_k_indices = torch.topk(similarity_matrix, k=k, dim=1)\n",
    "\n",
    "    results = {}\n",
    "    for i, query_string in enumerate(query_strings):\n",
    "        top_profiles = [\n",
    "            (profile_strings[idx.item()], sim.item())\n",
    "            for idx, sim in zip(top_k_indices[i], top_k_values[i])\n",
    "        ]\n",
    "        results[query_string] = top_profiles\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "74b7fb35-0ed1-4ddd-953e-41a4879ac9ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T01:50:43.039261Z",
     "iopub.status.busy": "2024-06-28T01:50:43.039077Z",
     "iopub.status.idle": "2024-06-28T01:50:43.066736Z",
     "shell.execute_reply": "2024-06-28T01:50:43.066022Z",
     "shell.execute_reply.started": "2024-06-28T01:50:43.039242Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2228/2508866266.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_embeddings = torch.stack([torch.tensor(emb) for emb in query_dict.values()])\n"
     ]
    }
   ],
   "source": [
    "top_profiles = get_top_k_profiles(query_dict, profile_vector_dict, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7602a7c5-a464-4ac5-8ec1-9f1333a3a0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:41:48.137886Z",
     "iopub.status.busy": "2024-06-28T02:41:48.136960Z",
     "iopub.status.idle": "2024-06-28T02:41:48.144242Z",
     "shell.execute_reply": "2024-06-28T02:41:48.143016Z",
     "shell.execute_reply.started": "2024-06-28T02:41:48.137818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#top_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c6de6-7ce4-42b1-a085-ea2aa77258ee",
   "metadata": {},
   "source": [
    "# adot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327316f1-cd6d-4530-9c77-25fc0a4176b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:53:06.108689Z",
     "iopub.status.busy": "2024-06-28T02:53:06.107930Z",
     "iopub.status.idle": "2024-06-28T02:53:06.115343Z",
     "shell.execute_reply": "2024-06-28T02:53:06.114114Z",
     "shell.execute_reply.started": "2024-06-28T02:53:06.108630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tables(table_nm:str):\n",
    "    query = f\"\"\"\n",
    "        SELECT  luna_id,\n",
    "                adot_profile\n",
    "        FROM x1113099.{table_nm}\n",
    "    \"\"\"\n",
    "    return bq_to_pandas(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ae13f7-12f8-4b9b-b794-7b825bce380b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:53:06.655427Z",
     "iopub.status.busy": "2024-06-28T02:53:06.654706Z",
     "iopub.status.idle": "2024-06-28T02:53:10.964784Z",
     "shell.execute_reply": "2024-06-28T02:53:10.963235Z",
     "shell.execute_reply.started": "2024-06-28T02:53:06.655369Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: exception on print statistics\n",
      "unsupported operand type(s) for /: 'NoneType' and 'int'\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "adot_df = get_tables(table_nm=\"user_retrieval_profile_adot_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecbd0e2-a876-45b1-b8b7-05d04933dd27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:53:10.967980Z",
     "iopub.status.busy": "2024-06-28T02:53:10.967285Z",
     "iopub.status.idle": "2024-06-28T02:53:11.621420Z",
     "shell.execute_reply": "2024-06-28T02:53:11.620380Z",
     "shell.execute_reply.started": "2024-06-28T02:53:10.967930Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile_dict = adot_df.set_index(\"luna_id\").to_dict()['adot_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d886a0-2dfd-4162-b0c3-e7290a67efd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:53:11.623133Z",
     "iopub.status.busy": "2024-06-28T02:53:11.622881Z",
     "iopub.status.idle": "2024-06-28T02:53:11.764211Z",
     "shell.execute_reply": "2024-06-28T02:53:11.763603Z",
     "shell.execute_reply.started": "2024-06-28T02:53:11.623107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = []\n",
    "luna_ids = []\n",
    "for luna_id, profile in profile_dict.items():\n",
    "    profiles.append(profile)\n",
    "    luna_ids.append(luna_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be189af-2329-47d0-a41c-ca581d8a5195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:53:14.125870Z",
     "iopub.status.busy": "2024-06-28T02:53:14.125095Z",
     "iopub.status.idle": "2024-06-28T02:58:31.735585Z",
     "shell.execute_reply": "2024-06-28T02:58:31.734455Z",
     "shell.execute_reply.started": "2024-06-28T02:53:14.125809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_result, luna_id_result = batch_embedd(profiles, luna_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2ef6f5-b91c-4197-9576-10d0934e3950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T05:09:10.249161Z",
     "iopub.status.busy": "2024-06-28T05:09:10.248652Z",
     "iopub.status.idle": "2024-06-28T05:09:11.096100Z",
     "shell.execute_reply": "2024-06-28T05:09:11.094813Z",
     "shell.execute_reply.started": "2024-06-28T05:09:10.249122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(luna_id_result)):\n",
    "    result.append({\"profile_text\": profiles[i], \"profile_vector\": embedding_result[i].squeeze(0), \"index\":luna_id_result[i], 'source_domain':'adot', 'dt':'temp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c87d675c-e3da-4634-9fe8-9ec63a76b80a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T05:09:23.534560Z",
     "iopub.status.busy": "2024-06-28T05:09:23.533608Z",
     "iopub.status.idle": "2024-06-28T05:09:24.015592Z",
     "shell.execute_reply": "2024-06-28T05:09:24.014845Z",
     "shell.execute_reply.started": "2024-06-28T05:09:23.534493Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2821c2d1-709e-46da-9a6f-033d0851dbed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T05:10:10.846576Z",
     "iopub.status.busy": "2024-06-28T05:10:10.845839Z",
     "iopub.status.idle": "2024-06-28T05:10:10.852414Z",
     "shell.execute_reply": "2024-06-28T05:10:10.851119Z",
     "shell.execute_reply.started": "2024-06-28T05:10:10.846517Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"skt-datahub\"\n",
    "db_name = \"adot_reco_dev\"\n",
    "table_name = \"adot_profile_vector_temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac1e188-a170-450c-af61-db64fae972d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T05:10:12.466017Z",
     "iopub.status.busy": "2024-06-28T05:10:12.465216Z",
     "iopub.status.idle": "2024-06-28T05:12:41.686391Z",
     "shell.execute_reply": "2024-06-28T05:12:41.684529Z",
     "shell.execute_reply.started": "2024-06-28T05:10:12.465957Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas_to_bq(pd_df = df, destination=f\"{PROJECT_ID}.{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1256ab7-1af9-4681-ba0b-a3a136b44f2d",
   "metadata": {},
   "source": [
    "# Tmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2d4f819-db58-47d0-86c5-b02893c75c8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:57:50.324587Z",
     "iopub.status.busy": "2024-06-28T15:57:50.324007Z",
     "iopub.status.idle": "2024-06-28T15:57:50.330221Z",
     "shell.execute_reply": "2024-06-28T15:57:50.329393Z",
     "shell.execute_reply.started": "2024-06-28T15:57:50.324546Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tables(table_nm:str):\n",
    "    query = f\"\"\"\n",
    "        SELECT  A.luna_id,\n",
    "                A.tmap_profile\n",
    "        FROM x1113099.{table_nm} AS A\n",
    "        \n",
    "        INNER JOIN \n",
    "        (\n",
    "            SELECT luna_id\n",
    "            FROM adot_reco.onemodelV3_input_inference_prd\n",
    "            WHERE dt = '2024-06-27'\n",
    "        ) AS B\n",
    "        ON A.luna_id = B.luna_id\n",
    "    \"\"\"\n",
    "    return bq_to_pandas(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90982f12-f6e4-4f5e-84c8-0bd6459eae44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:57:50.620728Z",
     "iopub.status.busy": "2024-06-28T15:57:50.620018Z",
     "iopub.status.idle": "2024-06-28T15:57:57.063752Z",
     "shell.execute_reply": "2024-06-28T15:57:57.063037Z",
     "shell.execute_reply.started": "2024-06-28T15:57:50.620670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: exception on print statistics\n",
      "unsupported operand type(s) for /: 'NoneType' and 'int'\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "tmap_df = get_tables(table_nm=\"user_retrieval_profile_tmap_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fbfc43b-cc26-4159-af6a-997d50922294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:55:10.056907Z",
     "iopub.status.busy": "2024-06-28T15:55:10.056443Z",
     "iopub.status.idle": "2024-06-28T15:55:10.360990Z",
     "shell.execute_reply": "2024-06-28T15:55:10.360334Z",
     "shell.execute_reply.started": "2024-06-28T15:55:10.056875Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299844"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmap_df.luna_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "236dafe4-76db-4938-9f6f-e969124bb531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:57:57.957678Z",
     "iopub.status.busy": "2024-06-28T15:57:57.956967Z",
     "iopub.status.idle": "2024-06-28T15:57:59.068567Z",
     "shell.execute_reply": "2024-06-28T15:57:59.067864Z",
     "shell.execute_reply.started": "2024-06-28T15:57:57.957620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile_dict = tmap_df.set_index(\"luna_id\").to_dict()['tmap_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "777776ab-f410-4d7e-875a-4566a5cc8081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:58:57.013470Z",
     "iopub.status.busy": "2024-06-28T15:58:57.012678Z",
     "iopub.status.idle": "2024-06-28T15:58:57.025242Z",
     "shell.execute_reply": "2024-06-28T15:58:57.024326Z",
     "shell.execute_reply.started": "2024-06-28T15:58:57.013405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_embedd(profiles:list, luna_ids:list, batch_size = 128, method=\"cls\"):\n",
    "    embedding_result = []\n",
    "    luna_id_result = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(profiles), batch_size):\n",
    "            batch = profiles[i:i+batch_size]\n",
    "            batch_luna_ids = luna_ids[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            #embeddings, _ = model(**inputs, return_dict=False)\n",
    "            if method =='cls':\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]  \n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "            elif method =='mean_pool':\n",
    "                embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings = embeddings.unsqueeze(1) # Shape: (batch_size, 768)\n",
    "            embedding_result.extend(embeddings.cpu().tolist())\n",
    "            luna_id_result.extend(batch_luna_ids)\n",
    "            \n",
    "    return embedding_result, luna_id_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "050f4370-39e0-43ac-a0e6-ce1d3c13e3c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:58:58.141941Z",
     "iopub.status.busy": "2024-06-28T15:58:58.141148Z",
     "iopub.status.idle": "2024-06-28T15:58:58.540454Z",
     "shell.execute_reply": "2024-06-28T15:58:58.539740Z",
     "shell.execute_reply.started": "2024-06-28T15:58:58.141880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = []\n",
    "luna_ids = []\n",
    "for luna_id, profile in profile_dict.items():\n",
    "    profiles.append(profile)\n",
    "    luna_ids.append(luna_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9557f7b5-d4b1-42af-afae-45bcbcb0d4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:58:58.542321Z",
     "iopub.status.busy": "2024-06-28T15:58:58.541965Z",
     "iopub.status.idle": "2024-06-28T16:20:13.809016Z",
     "shell.execute_reply": "2024-06-28T16:20:13.807964Z",
     "shell.execute_reply.started": "2024-06-28T15:58:58.542291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_result, luna_id_result = batch_embedd(profiles, luna_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1650ae91-5b47-401a-927e-53c2da0e5d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T16:21:54.726945Z",
     "iopub.status.busy": "2024-06-28T16:21:54.726454Z",
     "iopub.status.idle": "2024-06-28T16:22:30.227699Z",
     "shell.execute_reply": "2024-06-28T16:22:30.226986Z",
     "shell.execute_reply.started": "2024-06-28T16:21:54.726911Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(luna_id_result)):\n",
    "    result.append({\"profile_text\": profiles[i], \"profile_vector\": embedding_result[i][0], \"index\":luna_id_result[i], 'source_domain':'tmap', 'dt':'temp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9cc47bb5-2abd-4039-b39c-1d942d533b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:05:54.217614Z",
     "iopub.status.busy": "2024-06-28T17:05:54.216887Z",
     "iopub.status.idle": "2024-06-28T17:05:54.223139Z",
     "shell.execute_reply": "2024-06-28T17:05:54.222090Z",
     "shell.execute_reply.started": "2024-06-28T17:05:54.217556Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"skt-datahub\"\n",
    "db_name = \"adot_reco_dev\"\n",
    "table_name = \"tmap_profile_vector_temps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48344c9e-34b5-4236-b0c6-e6d858031694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:04:50.088095Z",
     "iopub.status.busy": "2024-06-28T17:04:50.087387Z",
     "iopub.status.idle": "2024-06-28T17:04:50.096794Z",
     "shell.execute_reply": "2024-06-28T17:04:50.095896Z",
     "shell.execute_reply.started": "2024-06-28T17:04:50.088036Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bigquery_schema = [\n",
    "    bigquery.SchemaField(\"profile_text\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"profile_vector\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"index\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"source_domain\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"dt\", \"STRING\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"profile_vector\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"profile_text\", StringType(), True),\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"source_domain\", StringType(), True),\n",
    "    StructField(\"dt\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "725d6413-3cb4-4df9-8e22-eb9934826f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:06:22.029993Z",
     "iopub.status.busy": "2024-06-28T17:06:22.029188Z",
     "iopub.status.idle": "2024-06-28T17:06:22.288149Z",
     "shell.execute_reply": "2024-06-28T17:06:22.286881Z",
     "shell.execute_reply.started": "2024-06-28T17:06:22.029931Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_ref = client.dataset(db_name).table(table_name)\n",
    "table = bigquery.Table(table_ref, schema=bigquery_schema)\n",
    "table = client.create_table(table, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db791826-cb16-43e4-9aae-6fb075cebf52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:06:55.572365Z",
     "iopub.status.busy": "2024-06-28T17:06:55.571567Z",
     "iopub.status.idle": "2024-06-28T17:07:07.894503Z",
     "shell.execute_reply": "2024-06-28T17:07:07.893578Z",
     "shell.execute_reply.started": "2024-06-28T17:06:55.572286Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 02:06:56 WARN TaskSetManager: Stage 6 contains a task of very large size (85075 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o233.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:69)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:111)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:532)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.scala:87)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:66)\n\t... 43 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_to_bq_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madot_reco_dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmap_profile_vector_temps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m, in \u001b[0;36mdf_to_bq_table\u001b[0;34m(df, dataset, table_name, partition, mode)\u001b[0m\n\u001b[1;32m      5\u001b[0m key \u001b[38;5;241m=\u001b[39m get_secrets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp/sktaic-datahub/dataflow\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msktaic-datahub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb64encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporaryGcsBucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp-seoul-7d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:738\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o233.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:69)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:111)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:532)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.scala:87)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:66)\n\t... 43 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 08:07:21 WARN Dispatcher: Message RemoteProcessDisconnected(10.40.84.161:4122) dropped. Could not find BlockManagerEndpoint1.\n"
     ]
    }
   ],
   "source": [
    "df_to_bq_table(df=df, dataset=\"adot_reco_dev\", table_name=\"tmap_profile_vector_temps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc392a64-b4c0-4c11-85fc-83f49c234797",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(chunk_list))):\n",
    "    chunk = chunk_list[i]\n",
    "    df = spark.createDataFrame(chunk, schema)\n",
    "    df_to_bq_table(df=df, dataset=\"adot_reco_dev\", table_name=\"tmap_profile_vector_temps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14e631a7-83c1-44bd-8a65-ce97eeb09c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:02:12.821323Z",
     "iopub.status.busy": "2024-06-28T17:02:12.820619Z",
     "iopub.status.idle": "2024-06-28T17:02:13.796027Z",
     "shell.execute_reply": "2024-06-28T17:02:13.795321Z",
     "shell.execute_reply.started": "2024-06-28T17:02:12.821251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = get_bigquery_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d62670ab-60d3-414f-9fc0-fb1d1cf64585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:02:13.797614Z",
     "iopub.status.busy": "2024-06-28T17:02:13.797340Z",
     "iopub.status.idle": "2024-06-28T17:02:15.012091Z",
     "shell.execute_reply": "2024-06-28T17:02:15.011374Z",
     "shell.execute_reply.started": "2024-06-28T17:02:13.797585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(luna_id_result)):\n",
    "    result.append({\"profile_text\": profiles[i], \"profile_vector\": embedding_result[i][0], \"index\":luna_id_result[i], 'source_domain':'tmap', 'dt':'temp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f7095ae-1f97-41b3-bf45-0e79b71fceb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:02:15.014044Z",
     "iopub.status.busy": "2024-06-28T17:02:15.013709Z",
     "iopub.status.idle": "2024-06-28T17:02:15.028188Z",
     "shell.execute_reply": "2024-06-28T17:02:15.027534Z",
     "shell.execute_reply.started": "2024-06-28T17:02:15.014021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "chunk_list = [result[i:i + chunk_size] for i in range(0, len(result), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84fc4906-8f70-44a6-b44f-72d8f50cb425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:04:09.524289Z",
     "iopub.status.busy": "2024-06-28T17:04:09.523565Z",
     "iopub.status.idle": "2024-06-28T17:04:09.531754Z",
     "shell.execute_reply": "2024-06-28T17:04:09.530343Z",
     "shell.execute_reply.started": "2024-06-28T17:04:09.524229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_vector: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- profile_text: string (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      " |-- source_domain: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b947f5-68a9-4473-8934-8bf243309759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23b4dce3-e15b-41e2-85ec-8e665bad1fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T17:02:40.037182Z",
     "iopub.status.busy": "2024-06-28T17:02:40.036464Z",
     "iopub.status.idle": "2024-06-28T17:03:28.575731Z",
     "shell.execute_reply": "2024-06-28T17:03:28.574806Z",
     "shell.execute_reply.started": "2024-06-28T17:02:40.037125Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db75dde8c4544bf889bc1c2dd1be96ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 02:03:15 WARN TaskSetManager: Stage 4 contains a task of very large size (85075 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o223.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:69)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:111)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:532)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.scala:87)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:66)\n\t... 43 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m chunk \u001b[38;5;241m=\u001b[39m chunk_list[i]\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(chunk, schema)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdf_to_bq_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madot_reco_dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmap_profile_vector_temps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m, in \u001b[0;36mdf_to_bq_table\u001b[0;34m(df, dataset, table_name, partition, mode)\u001b[0m\n\u001b[1;32m      5\u001b[0m key \u001b[38;5;241m=\u001b[39m get_secrets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcp/sktaic-datahub/dataflow\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msktaic-datahub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb64encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporaryGcsBucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp-seoul-7d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:738\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o223.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:69)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:111)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:532)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.scala:87)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:66)\n\t... 43 more\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(chunk_list))):\n",
    "    chunk = chunk_list[i]\n",
    "    df = spark.createDataFrame(chunk, schema)\n",
    "    \n",
    "    #df_to_bq_table(df=df, dataset=\"adot_reco_dev\", table_name=\"tmap_profile_vector_temps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc40a9-80de-453b-b5c0-908a0b9cdb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfe67b-948d-45ed-a03d-efbe144ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99dcd5cf-3210-4876-a8f7-8aedb921ca2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T16:58:12.904561Z",
     "iopub.status.busy": "2024-06-28T16:58:12.903786Z",
     "iopub.status.idle": "2024-06-28T16:58:12.913518Z",
     "shell.execute_reply": "2024-06-28T16:58:12.912592Z",
     "shell.execute_reply.started": "2024-06-28T16:58:12.904499Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_bq_table(df, dataset, table_name, partition=None, mode=\"append\"):\n",
    "    import base64\n",
    "    from skt.vault_utils import get_secrets\n",
    "\n",
    "    key = get_secrets(\"gcp/sktaic-datahub/dataflow\")[\"config\"]\n",
    "    table = f\"{dataset}.{table_name}${partition}\" if partition else f\"{dataset}.{table_name}\"\n",
    "    df.write.format(\"bigquery\").option(\"project\", \"sktaic-datahub\").option(\n",
    "        \"credentials\", base64.b64encode(key.encode()).decode()\n",
    "    ).option(\"table\", table).option(\"temporaryGcsBucket\", \"temp-seoul-7d\").save(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18560fc-53dd-436b-baaa-4ba8f6f52367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04494a0-4b15-4af3-8202-83fc888f63d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c04dc-4e50-4b48-a440-b2786b23b486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e14627ac-1a4d-440d-bd52-bd4a08ada204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:46:56.146180Z",
     "iopub.status.busy": "2024-06-28T15:46:56.145480Z",
     "iopub.status.idle": "2024-06-28T15:46:56.152514Z",
     "shell.execute_reply": "2024-06-28T15:46:56.151281Z",
     "shell.execute_reply.started": "2024-06-28T15:46:56.146120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, DoubleType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b10d2fdd-3865-49fe-8ab6-fc9136e46208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T16:39:43.864655Z",
     "iopub.status.busy": "2024-06-28T16:39:43.863766Z",
     "iopub.status.idle": "2024-06-28T16:39:43.872015Z",
     "shell.execute_reply": "2024-06-28T16:39:43.871024Z",
     "shell.execute_reply.started": "2024-06-28T16:39:43.864585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"profile_vector\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"profile_text\", StringType(), True),\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"source_domain\", StringType(), True),\n",
    "    StructField(\"dt\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52756996-ad72-43d5-bf6d-7c4f91b1c619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T16:39:38.586658Z",
     "iopub.status.busy": "2024-06-28T16:39:38.586010Z",
     "iopub.status.idle": "2024-06-28T16:39:38.606405Z",
     "shell.execute_reply": "2024-06-28T16:39:38.604949Z",
     "shell.execute_reply.started": "2024-06-28T16:39:38.586621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e98e57ae-ce55-4ee6-b088-2ee83e20be95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T16:39:45.509887Z",
     "iopub.status.busy": "2024-06-28T16:39:45.509387Z",
     "iopub.status.idle": "2024-06-28T16:47:51.794021Z",
     "shell.execute_reply": "2024-06-28T16:47:51.791503Z",
     "shell.execute_reply.started": "2024-06-28T16:39:45.509857Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:526\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m    525\u001b[0m data \u001b[38;5;241m=\u001b[39m [schema\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, schema\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/context.py:574\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateRDDServer\u001b[39m():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 574\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/context.py:614\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;66;03m# we eagerly reads the file so we can delete right after.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(tempFile\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/context.py:569\u001b[0m, in \u001b[0;36mSparkContext.parallelize.<locals>.reader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename):\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadRDDFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSlices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(result, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e351932f-6c32-47b7-9f3d-4c820e613a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T15:47:53.366293Z",
     "iopub.status.busy": "2024-06-28T15:47:53.365584Z",
     "iopub.status.idle": "2024-06-28T15:47:53.389712Z",
     "shell.execute_reply": "2024-06-28T15:47:53.388470Z",
     "shell.execute_reply.started": "2024-06-28T15:47:53.366234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_vector: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- profile_text: string (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      " |-- source_domain: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a16ad5-a929-4b5b-9b0e-9dfb9a1307da",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-28T10:08:41.078419Z",
     "iopub.status.idle": "2024-06-28T10:08:41.078844Z",
     "shell.execute_reply": "2024-06-28T10:08:41.078728Z",
     "shell.execute_reply.started": "2024-06-28T10:08:41.078714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"skt-datahub\"\n",
    "db_name = \"adot_reco_dev\"\n",
    "table_name = \"tmap_profile_vector_temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b912eb-f019-466f-9281-061a78567784",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-28T10:08:41.079703Z",
     "iopub.status.idle": "2024-06-28T10:08:41.079941Z",
     "shell.execute_reply": "2024-06-28T10:08:41.079831Z",
     "shell.execute_reply.started": "2024-06-28T10:08:41.079818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_to_bq_table(df=spark_df, dataset='adot_reco_dev', table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7555b3-c163-40b9-9623-f73215a9d572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
